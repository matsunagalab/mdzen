{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Clarification - User Requirements and Brief Generation\n",
    "\n",
    "**Goal**: Clarify user requirements and transform them into a structured `SimulationBrief`.\n",
    "\n",
    "This notebook implements the first phase of our MD setup workflow:\n",
    "\n",
    "1. **User Clarification** - Determines if additional information is needed\n",
    "2. **Brief Generation** - Transforms the conversation into a structured simulation brief\n",
    "\n",
    "Based on `deep_research_from_scratch/notebooks/1_scoping.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and set up auto-reload\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Schemas\n",
    "\n",
    "First, we'll define the state objects and schemas for our clarification process.\n",
    "\n",
    "The state object stores and passes context between different phases.\n",
    "\n",
    "**Note**: We use `%%writefile` to save code to files. This allows us to reuse it in future notebooks and creates deployable code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/mcp_md/state_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/mcp_md/state_scope.py\n",
    "\n",
    "\"\"\"State Definitions and Pydantic Schemas for Clarification Phase.\n",
    "\n",
    "This module defines state objects and structured schemas for the MD setup workflow\n",
    "using LangGraph 1.0+ patterns:\n",
    "- MessagesState for conversation tracking\n",
    "- Annotated fields with proper reducers\n",
    "- Pydantic models for structured outputs\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Optional, Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# ===== STATE DEFINITIONS (LangGraph 1.0+) =====\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state for the full agent - only contains messages from user input.\n",
    "    \n",
    "    Used as input_schema for the main graph to define the public input interface.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"Main state for the full multi-phase MD setup system.\n",
    "    \n",
    "    Extends MessagesState with additional fields for MD setup coordination.\n",
    "    All fields use proper reducers for state updates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Phase 1: Clarification\n",
    "    research_brief: Optional[str] = None  # Compatibility with deep_research pattern\n",
    "    simulation_brief: Optional[\"SimulationBrief\"] = None\n",
    "\n",
    "    # Phase 2: Setup (will be added in Notebook 2)\n",
    "    setup_messages: Annotated[Sequence[BaseMessage], add_messages] = []\n",
    "    decision_log: Annotated[list[dict], operator.add] = []\n",
    "    outputs: dict = {}\n",
    "\n",
    "    # Phase 3: Validation & Export (will be added in Notebook 4)\n",
    "    qc_results: dict = {}\n",
    "    exports: dict = {}\n",
    "    final_report: str = \"\"\n",
    "\n",
    "\n",
    "# ===== STRUCTURED OUTPUT SCHEMAS (Pydantic) =====\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decision and questions.\n",
    "    \n",
    "    Used with .with_structured_output() for LLM decision making.\n",
    "    \"\"\"\n",
    "\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the simulation requirements.\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verification message that we will start setup after user provides information.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class SimulationBrief(BaseModel):\n",
    "    \"\"\"Schema for structured simulation brief generation.\n",
    "    \n",
    "    Transforms conversation into structured MD setup parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Structure\n",
    "    pdb_id: Optional[str] = Field(default=None, description=\"PDB ID (e.g., 1ABC)\")\n",
    "    fasta_sequence: Optional[str] = Field(\n",
    "        default=None, description=\"FASTA sequence for de novo generation\"\n",
    "    )\n",
    "    ligand_smiles: Optional[str] = Field(default=None, description=\"Ligand SMILES string\")\n",
    "\n",
    "    # Simulation parameters\n",
    "    ph: float = Field(default=7.0, description=\"pH value\")\n",
    "    salt_concentration: float = Field(default=0.15, description=\"Salt concentration (M)\")\n",
    "    water_model: str = Field(default=\"TIP3P\", description=\"Water model\")\n",
    "    box_padding: float = Field(default=12.0, description=\"Box padding (Ã…)\")\n",
    "    force_field: str = Field(default=\"ff19SB\", description=\"Protein force field\")\n",
    "\n",
    "    # Workflow preferences\n",
    "    use_boltz2_docking: bool = Field(default=True, description=\"Use Boltz-2 for docking\")\n",
    "    refine_with_smina: bool = Field(default=False, description=\"Refine with Smina\")\n",
    "    output_formats: list[str] = Field(default=[\"amber\"], description=\"Output formats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Define prompts for clarification and brief generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/mcp_md/prompts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/mcp_md/prompts.py\n",
    "\n",
    "\"\"\"Prompt templates for the MD setup system.\n",
    "\n",
    "This module contains all prompt templates used across the workflow components.\n",
    "\"\"\"\n",
    "\n",
    "clarify_requirements_prompt = \"\"\"\n",
    "These are the messages exchanged so far with the user requesting MD setup:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Assess whether you need to ask a clarifying question, or if the user has provided enough information to start MD system setup.\n",
    "\n",
    "Required information for MD setup:\n",
    "- Protein structure (PDB ID or FASTA sequence or PDB file or CIF file)\n",
    "- Ligand (SMILES string) if protein-ligand complex or ligand-only system\n",
    "\n",
    "If clarification needed, return:\n",
    "{{\"need_clarification\": true, \"question\": \"<your question>\", \"verification\": \"\"}}\n",
    "\n",
    "If no clarification needed, return:\n",
    "{{\"need_clarification\": false, \"question\": \"\", \"verification\": \"<acknowledgement>\"}}\n",
    "\"\"\"\n",
    "\n",
    "generate_simulation_brief_prompt = \"\"\"\n",
    "Extract all simulation requirements from these messages:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Return a structured JSON with: pdb_id, fasta_sequence, ligand_smiles, ph, salt_concentration, water_model, box_padding, force_field, use_boltz2_docking, refine_with_smina, output_formats.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarification Agent\n",
    "\n",
    "Now we implement the clarification workflow with two nodes:\n",
    "1. `clarify_requirements` - Determines if more information is needed\n",
    "2. `generate_simulation_brief` - Creates structured brief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/mcp_md/clarification_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/mcp_md/clarification_agent.py\n",
    "\n",
    "\"\"\"User Clarification and Simulation Brief Generation.\n",
    "\n",
    "This module implements the clarification phase of MD setup using LangGraph 1.0+ patterns:\n",
    "- Command API for node routing\n",
    "- Structured outputs with Pydantic\n",
    "- MessagesState-based state management\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command\n",
    "\n",
    "from mcp_md.prompts import (\n",
    "    clarify_requirements_prompt,\n",
    "    generate_simulation_brief_prompt,\n",
    ")\n",
    "from mcp_md.state_scope import (\n",
    "    AgentInputState,\n",
    "    AgentState,\n",
    "    ClarifyWithUser,\n",
    "    SimulationBrief,\n",
    ")\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date formatted for prompts.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %-d, %Y\")\n",
    "\n",
    "# Initialize model (LangGraph 1.0+ compatible)\n",
    "# Option 1: Ollama (local) - Note: gemma3:4b doesn't exist. Using gemma2:9b\n",
    "# model = ChatOllama(model=\"gemma3:12b\", temperature=0.0)\n",
    "\n",
    "# Option 2: OpenAI-compatible API (uncomment to use)\n",
    "model = init_chat_model(model=\"openai:gpt-4o\", temperature=0.0)\n",
    "\n",
    "def clarify_requirements(\n",
    "    state: AgentState,\n",
    ") -> Command[Literal[\"generate_simulation_brief\", \"__end__\"]]:\n",
    "    \"\"\"Determine if sufficient information exists to proceed with MD setup.\n",
    "    \n",
    "    Returns:\n",
    "        Command: Routes to END if clarification needed, otherwise to generate_simulation_brief\n",
    "    \"\"\"\n",
    "    structured_model = model.with_structured_output(ClarifyWithUser)\n",
    "    response = structured_model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=clarify_requirements_prompt.format(\n",
    "                    messages=get_buffer_string(messages=state[\"messages\"]),\n",
    "                    date=get_today_str(),\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if response.need_clarification:\n",
    "        # Need more info - return to user\n",
    "        return Command(\n",
    "            goto=END, update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        # Have enough info - proceed to brief generation\n",
    "        return Command(\n",
    "            goto=\"generate_simulation_brief\",\n",
    "            update={\"messages\": [AIMessage(content=response.verification)]},\n",
    "        )\n",
    "\n",
    "def generate_simulation_brief(state: AgentState) -> dict:\n",
    "    \"\"\"Generate structured simulation brief from conversation history.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated state with simulation_brief, research_brief, and setup_messages\n",
    "    \"\"\"\n",
    "    structured_model = model.with_structured_output(SimulationBrief)\n",
    "    response = structured_model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=generate_simulation_brief_prompt.format(\n",
    "                    messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "                    date=get_today_str(),\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"simulation_brief\": response,\n",
    "        \"research_brief\": str(response.model_dump()),\n",
    "        \"setup_messages\": [\n",
    "            HumanMessage(content=f\"Starting MD setup with: {response.model_dump_json()}\")\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# Build clarification graph (LangGraph 1.0+ pattern)\n",
    "clarification_builder = StateGraph(AgentState, input_schema=AgentInputState)\n",
    "clarification_builder.add_node(\"clarify_requirements\", clarify_requirements)\n",
    "clarification_builder.add_node(\"generate_simulation_brief\", generate_simulation_brief)\n",
    "clarification_builder.add_edge(START, \"clarify_requirements\")\n",
    "clarification_builder.add_edge(\"generate_simulation_brief\", END)\n",
    "clarification_graph = clarification_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-md",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
